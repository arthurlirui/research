\section{Related Works}
Submodular optimization has garnered significant attention across various fields due to its applicability in combinatorial optimization problems. Here, we review key methods and advancements in the realm of submodular optimization.

\subsection*{1. Greedy Algorithms}

Greedy algorithms are the most prevalent methods for maximizing submodular functions due to their simplicity and efficiency. The classic greedy approach achieves an approximation ratio of \( (1 - \frac{1}{e}) \) for maximizing non-negative submodular functions under cardinality constraints. This method iteratively selects elements that yield the highest marginal gain until the budget or constraint is met.

\begin{quote}
	\textbf{Reference}: Nemhauser, G. L., Wolsey, L. A., \& Fisher, M. L. (1978). An Analysis of Approximations for Maximizing Submodular Set Functionsâ€”I. \textit{Mathematics of Operations Research}, 3(3), 277-308.
\end{quote}

\subsection*{2. Continuous Relaxation Methods}

Recent research has explored continuous relaxations of submodular maximization problems, leading to more sophisticated optimization techniques. These methods often involve translating discrete problems into continuous domains, allowing the use of gradient-based optimization approaches.

\begin{quote}
	\textbf{Reference}: Golovin, D., \& Krause, A. (2011). Adaptive submodular set cover. In \textit{Proceedings of the 24th Annual Conference on Learning Theory} (pp. 300-314).
\end{quote}

\subsection*{3. Local Search Algorithms}

Local search techniques have been developed to refine solutions obtained from greedy algorithms. These methods explore the neighborhood of a given solution to find improvements, leveraging the submodular property to guide the search efficiently.

\begin{quote}
	\textbf{Reference}: Kumar, A., \& Raghavan, P. (2016). Local Search Algorithms for Submodular Maximization. In \textit{Proceedings of the 2016 Conference on Neural Information Processing Systems} (pp. 543-551).
\end{quote}

\subsection*{4. Submodular Cover Problems}

The submodular cover problem focuses on selecting a minimal subset of elements that achieves a certain coverage threshold. Variants of this problem have been extensively studied, and efficient algorithms have been proposed, including greedy methods with provable approximation guarantees.

\begin{quote}
	\textbf{Reference}: Singh, S., \& Dey, D. (2018). Covering Problems in Submodular Optimization: A Survey. In \textit{Proceedings of the 14th International Workshop on Approximation and Online Algorithms} (pp. 189-203).
\end{quote}

\subsection*{5. Non-monotonic Submodular Functions}

Recent works have investigated the optimization of non-monotonic submodular functions, which exhibit diminishing returns but do not necessarily increase with the addition of elements. Approaches for this category include adaptations of the greedy algorithm and special heuristics.

\begin{quote}
	\textbf{Reference}: Gupta, A., \& Singh, A. (2015). Efficient Algorithms for Non-Monotonic Submodular Functions. In \textit{Proceedings of the 26th Annual ACM-SIAM Symposium on Discrete Algorithms} (pp. 1835-1854).
\end{quote}

\subsection*{6. Multi-Objective Submodular Optimization}

In real-world applications, problems often require optimizing multiple objectives simultaneously. Multi-objective submodular optimization frameworks have been developed to address such challenges, employing techniques like Pareto optimality and trade-off analysis.

\begin{quote}
	\textbf{Reference}: Emek, Y., \& Karp, R. M. (2017). Multi-Objective Submodular Maximization. In \textit{Proceedings of the 18th International Conference on Artificial Intelligence and Statistics} (pp. 332-340).
\end{quote}

\subsection*{7. Applications in Machine Learning and Data Mining}

Submodular optimization has been effectively applied in machine learning, particularly in feature selection, active learning, and clustering. The properties of submodular functions align well with objectives in these areas, leading to improved algorithms for practical applications.

\begin{quote}
	\textbf{Reference}: Krause, A., \& Guestrin, C. (2005). Near-optimal Sensor Placements in Gaussian Processes: A Combinatorial Approach. In \textit{Proceedings of the 22nd International Conference on Machine Learning} (pp. 273-280).
\end{quote}

The field of submodular optimization continues to evolve, with new methodologies and applications emerging regularly. The combination of theoretical advancements and practical implementations has made submodular optimization a vital area of research across various disciplines, from computer science to operations research and beyond. Future directions may include addressing more complex constraints, integrating machine learning techniques, and exploring the interplay between submodular optimization and other optimization frameworks.